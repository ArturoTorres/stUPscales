#'---------------------------------------------------------------------------------------------
#+ inference, echo=TRUE, include=TRUE
#' Now, the BN is ready and we can start inferring from the network.
cpquery(fittedbn, event = (Proteins=="<3"), evidence = ( Smoking=="no") )
#' which results in 0.61. Note that although the Proteins variable
#' is conditioned on 2 variables, we did the query based on the
#' available evidence on only one variables.
#' But let make our evidence richer by asking the following:
#' What is the chance that a non-smoker with pressure greater
#' than 140 has a Proteins level less than 3?
cpquery(fittedbn, event = (Proteins=="<3"),
evidence = ( Smoking=="no" & Pressure==">140" ) )
#' Actual data UK
#'---------------------------------------------------------------------------------------------
#+ actual_data, echo=TRUE, include=TRUE
time  <- seq(from = 1990, to = 2015, by = 1)
set.seed(70)
var1 <- runif(n = length(time), min = 15, max = 50)
var1
var2 <- rnorm(n = length(time), mean = 100, sd = 0.15)
var2
var3 <- runif(n = length(time), min = 15, max = 100)
var3
var4 <- var2 * 10
var4
var5 <- var1 * 2
var5
var6 <- var1 * -2
var6
My.var <- function(var.1){
#' annual change (percentage)
diff <- diff(var.1)
perc <- diff/var.1[1:(length(var.1)-1)]*100
check <- matrix(data=NA, nrow=length(perc), ncol=1)
for(i in 1:(length(var.1)-1)){
if(perc[i] < 0){
check[i] <- var.1[i] - var.1[i]*(-perc[i]/100)
}
if(perc[i] >= 0){
check[i] <- var.1[i] + var.1[i]*(perc[i]/100)
}
}
data <- cbind.data.frame(time = time[1:length(time)-1],
data = var.1[1:length(time)-1],
perc = perc,
check = check)
data
range <- max(perc) - min(perc)
class <- matrix(data=NA, nrow=length(perc), ncol=1)
for(i in 1:length(perc)){
pos <- max(perc)/3
neg <- min(perc)/3
if(perc[i] > 0 & perc[i] < 1*pos){class[i] <- "slightly increase"}
else if(perc[i] >= 1*pos & perc[i] < 2*pos){class[i] <- "medium increase"}
else if(perc[i] >= 2*pos & perc[i] <= max(perc)){class[i] <- "high increase"}
else if(perc[i] < 0 & perc[i] > 1*neg){class[i] <- "slightly decrease"}
else if(perc[i] <= 1*neg & perc[i] > 2*neg){class[i] <- "medium decrease"}
else if(perc[i] <= 2*neg & perc[i] >= min(perc)){class[i] <- "high decrease"}
}
data <- cbind.data.frame(data, class=class)
data
}
#' e.g. red beef (number of cows in thousands)
(data.1 <- My.var(var1))
#' e.g. GDP  (in billions of dollars)
(data.2 <- My.var(var2))
#' e.g. arable land (in million Ha)
(data.3 <- My.var(var3))
#' e.g. arable crops1 (in Kg/Ha)
(data.4 <- My.var(var4))
#' e.g. arable crops2 (in Kg/Ha)
(data.5 <- My.var(var5))
#' e.g. arable crops3 (in Kg/Ha)
(data.6 <- My.var(var6))
data <- cbind.data.frame(var1 = factor(data.1[,5]),
var2 = factor(data.2[,5]),
var3 = factor(data.3[,5]),
var4 = factor(data.4[,5]),
var5 = factor(data.5[,5]),
var6 = factor(data.6[,5]))
colnames(data) <- c("var1", "var2", "var3", "var4", "var5", "var6")
(my.res <- hc(data))
plot(my.res)
# (my.res <- tabu(data))
# plot(my.res)
my.fittedbn <- bn.fit(my.res, data = data)
print(my.fittedbn$var1)
print(my.fittedbn$var2)
print(my.fittedbn$var3)
print(my.fittedbn$var4)
print(my.fittedbn$var5)
print(my.fittedbn$var6)
cpquery(my.fittedbn, event = (var1=="high increase"),
evidence = ( var2=="medium decrease" & var3=="medium decrease"))
cpquery(my.fittedbn, event = (var1=="high increase"),
evidence = ( var4=="medium decrease" & var5=="medium decrease"))
cpquery(my.fittedbn, event = (var1=="high increase"),
evidence = ( var4=="medium decrease" & var5=="high increase"))
#' Actual data UK (continuos data)
#'==============================================================================
#+ actual_data_raw, echo=TRUE, include=TRUE
data.raw <- cbind.data.frame(var1 = var1,
var2 = var2,
var3 = var3,
var4 = var4,
var5 = var5,
var6 = var6)
data.raw
(my.res.raw <- hc(data.raw))
plot(my.res.raw)
var.model = VAR(data.raw, p = 1, type = "const")
var.model
#' The estimated coefficient contains many missing values (NA); therefore,
#' approaches allowing for dimension reduction are required to analyze these data.
#' We consider the following dimension reduction approaches:
#'
#' • L 1 norm penalty (LASSO)
#'
#' • James-Stein shrinkage
#'
#' • Low-order conditional dependencies approximation
#'
#' LASSO with lars:
# library(GeneNet)
# data(arth800)
# summary(arth800.expr)
# dim(arth800.expr)
#' The data contains 2 sets of 11 time points.
# variance = diag(var(arth800.expr))
# plot(sort(variance, decreasing = TRUE),
#        type = "l", ylab = "Variance")
# abline(h = 2, lty = 2)
# posVar2 = which(variance > 2)
# dataVar2 = arth800.expr[, posVar2]
# dim(dataVar2)
# head(dataVar2)
# dataVar2inline = dataVar2[c(seq(1, 22, by = 2),
#                             seq(2, 22, by = 2)), ]
# head(dataVar2inline)
# dim(dataVar2inline)
#
# data = dataVar2inline
# x = data[-c(21:22), ]
(data.raw <- as.matrix(data.raw[]))
x = as.matrix(data.raw)
dim(x)
min(x)
fit.all = lapply(colnames(data.raw),
function(variable) {
y = data.raw[, variable]
lars(y = y, x = x, type = "lasso")
})
fit.all
cv.pred.all = lapply(1:dim(data.raw)[2],
function(variable) {
y = data.raw[, variable]
lasso.cv = cv.lars(y = y, x = x,
mode = "fraction")
frac = lasso.cv$index[
which.min(lasso.cv$cv)]
predict(fit.all[[variable]], s = frac,
type = "coef", mode = "fraction")
})
DBNlasso = matrix(0, dim(data.raw)[2], dim(data.raw)[2])
for (i in 1:dim(DBNlasso)[1]) {
DBNlasso[i, ] =
cv.pred.all[i][[1]]$coefficients
}
# percentage of arcs
sum(DBNlasso != 0)
sum(DBNlasso != 0)/prod(dim(DBNlasso))
plot(sort(abs(DBNlasso), decr = TRUE),
type = "l",
ylab = "Absolute coefficients")
DBNlasso
library(corrplot)
correl <- cor(data.raw)
corrplot(correl, type = "lower", method = "circle")
data.noncorrel <- cbind.data.frame(var1 = factor(data.1[,5]),
var2 = factor(data.2[,5]),
var3 = factor(data.3[,5]))
(my.res.noncorrel <- hc(data.noncorrel))
plot(my.res.noncorrel)
my.fittedbn.noncorrel <- bn.fit(my.res.noncorrel, data = data.noncorrel)
print(my.fittedbn.noncorrel$var1)
print(my.fittedbn.noncorrel$var2)
print(my.fittedbn.noncorrel$var3)
cpquery(my.fittedbn.noncorrel, event = (var1=="high increase"),
evidence = ( var2=="medium decrease" & var3=="medium decrease"))
cpquery(my.fittedbn.noncorrel, event = (var2=="high increase"),
evidence = ( var1=="medium decrease" & var3=="medium decrease"))
#' Assigning dependencies
#'==============================================================================
#+ assign_dependencies, echo=TRUE, include=TRUE
my.res.noncorrel <- set.arc(x = my.res.noncorrel, from =  "var1", to = "var3")
my.res.noncorrel <- set.arc(x = my.res.noncorrel, from =  "var2", to = "var1")
my.res.noncorrel
plot(my.res.noncorrel)
my.fittedbn.noncorrel2 <- bn.fit(my.res.noncorrel, data = data.noncorrel)
print(my.fittedbn.noncorrel2$var1)
print(my.fittedbn.noncorrel2$var2)
print(my.fittedbn.noncorrel2$var3)
cpquery(my.fittedbn.noncorrel, event = (var1=="high increase"),
evidence = ( var2=="medium decrease" & var3=="medium decrease"))
cpquery(my.fittedbn.noncorrel, event = (var2=="high increase"),
evidence = ( var1=="medium decrease" & var3=="medium decrease"))
bn.graph <- graphviz.chart(my.fittedbn.noncorrel2)
log(-89)
log(89)
log(,001)
log(.001)
library(RefManageR)
sessionInfo()
library(knitr)
library(RefManageR)
library(tm)
folder.base   <- "~/Documents/02_working/3-Production/01_bibliography/03_R/searches/"
file.bibtex   <- "~/Documents/02_working/3-Production/01_bibliography/03_R/searches/WOS_PhD_general_introduction.bib"
folder.new    <- ""
file.bibentry.new  <- ""
entry.key2remove  <- "ISI:000272073800017"
bibtex.file <- ReadBib(file.bibtex, .Encoding = "UTF-8")
library(knitr)
library(RefManageR)
library(tm)
sessionInfo()
folder.base   <- "~/Documents/02_working/3-Production/01_bibliography/03_R/searches/"
file.bibtex   <- "~/Documents/02_working/3-Production/01_bibliography/03_R/searches/WOS_PhD_general_introduction.bib"
folder.new    <- ""
file.bibentry.new  <- ""
entry.key2remove  <- "ISI:000272073800017"
bibtex.file <- ReadBib(file.bibtex, .Encoding = "UTF-8")
load("~/Documents/02_working/3-Production/01_bibliography/03_R/searches/WoS_PhD_general_introduction/output/ws_WOS_PhD_general_introduction.RData")
load("~/Documents/02_working/3-Production/01_bibliography/03_R/searches/WoS_PhD_general_introduction/output/WOS_PhD_general_introduction_net-citatition-stat-recent.RData")
str(stats)
stats
cor(stats)
cor(stats[-1])
stats[order(cluster.res1["Ranking"]),
c("vertexID", "Ranking", "cluster")]
stats[order(stats["Ranking"]),
c("vertexID", "Ranking", "cluster")]
stats[order(stats["Ranking"]),
c("vertexID", "Ranking", "vertexAuthority", "cluster")]
stats[order(stats["vertexAuthority"]),
c("vertexID", "Ranking", "vertexAuthority", "cluster")]
stats[order(stats["vertexCentrEigen"]),
c("vertexID", "Ranking", "vertexCentrEigen", "cluster")]
#' #' Saves plot as png on the filesytem.
#' #'
#'
library(EOTSA)
data(PROBAV_smpl)
plot(x=c(1:10), y=c(1:10))
my_plot <- recordPlot()
plot.new() ## clean up device
my_plot # redraw
my_path <- "my_plot"
eotsa_save_plot(the_plot = my_plot, filepath = my_path)
eotsa_save_plot(the_plot = my_plot, filepath = my_path)
my_path <- "/home/atorres/Documents/02_working_post-doc/3-Production/05_models/03_EOTSA/unit_tests/plots"
my_path <- "/home/atorres/Documents/02_working_post-doc/3-Production/05_models/03_EOTSA/unit_tests/plots"
eotsa_save_plot(the_plot = my_plot, filepath = my_path)
my_path <- "/home/atorres/Documents/02_working_post-doc/3-Production/05_models/03_EOTSA/unit_tests/plots.png"
eotsa_save_plot(the_plot = my_plot, filepath = my_path)
library(EOTSA)
data(PROBAV_smpl)
plot(rs)
#' eotsa_arima: (function) Calculates ARIMA coefficients for a given STFDF or RasterStack/-Brick object using auto.arima.
#' -----------------------------------------------------------------------------------------------------------
#'
#' Config
local <- TRUE
startdate <- "2015-01-01"
enddate   <- "2018-12-31"
bbox      <- "4, 4.05, 51.95, 52"
#' Loading lybrary
library(EOTSA)
librray(testthat)
#' Loading lybrary
library(EOTSA)
library(testthat)
#' Defining bounding box
bbox <- as.numeric(strsplit(bbox, split=",", fixed=TRUE)[[1]])
#' Data import
if (local) {
# Loading rs dataset (a RasterBrick)
data(PROBAV_smpl)
} else {
rs <- loadPROBA(startdate, enddate, extent(bbox))
}
#' Loading lybrary
library(EOTSA)
library(spacetime)
setwd("~/Documents/02_working/3-Production/05_IV-Year/07_journal_papers/03_collaborative_Ben_Edzer/03_R-Scripts")
load("~/Documents/02_working/3-Production/05_IV-Year/07_journal_papers/03_collaborative_Ben_Edzer/03_R-Scripts/ws.RData")
data.xts <- Lux_precipitation_2010_2011
my.smoothed.ts <- Smooth.ts(data.xts, kernel = kernel("daniell", c(1, 1)), sm.threshold = 1)
#
# author: J.A. Torres-Matallana
# organization: Luxembourg Institute of Science and Technology (LIST), Belvaux, Luxembourg
# date: 01.10.2018 - 29.05.2019
#
# ====================================================================================================
#' data.xts     : xts, time series to smooth
#' kernel       : tskernel, the smoothing kernel
#' sm.threshold : numeric, just apply the Kernel smoothing to values lower than sm.threshold
# ====================================================================================================
Smooth.ts <- function(data.xts, kernel = kernel("daniell", c(1, 1)), sm.threshold = 1){
# Kernel smoothing of time series
kd <- kernel("daniell", c(1, 1))
plot(kd)
# removing the initial and two ending values because will be lost after smoothing
data1.xts <- data.xts[3:(nrow(data.xts)-2),] # TODO: generic kernel
data.xts.sm  <- data1.xts
for(i in 1:ncol(data.xts)){ # Kernel smoothing and NA filling
data.xts.sm[,i] <- kernapply(as.numeric(coredata(na.locf(data.xts[,i]))), kd)
}
# just apply the Kernel smoothing to values lower than sm.threshold
for(i in 1:ncol(data.xts.sm)){ # Kernel smoothing and NA filling
data.xts.sm[,i] <- ifelse(data.xts.sm[,i] < sm.threshold, data.xts.sm[,i],
as.numeric(coredata(data1.xts[,i])))
data.xts.sm[,i] <- ifelse(data.xts.sm[,i] < 0, 0, as.numeric(coredata(
data.xts.sm[,i])))
data.xts.sm[,i] <- na.locf(data.xts.sm[,i])
}
return(data.xts.sm)
}
my.smoothed.ts <- Smooth.ts(data.xts, kernel = kernel("daniell", c(1, 1)), sm.threshold = 1)
library(stUPscales)
# ====================================================================================================
# Smooth.ts
# ====================================================================================================
data("Lux_precipitation_2010_2011")
data.xts <- Lux_precipitation_2010_2011
my.smoothed.ts <- Smooth.ts(data.xts, kernel = kernel("daniell", c(1, 1)), sm.threshold = 1)
library(xts)
my.smoothed.ts <- Smooth.ts(data.xts, kernel = kernel("daniell", c(1, 1)), sm.threshold = 1)
# ====================================================================================================
# xts2STFDF
# ====================================================================================================
data("Lux_stations")
point = stations
data.xts = my.smoothed.ts
point$name
colnames(as.data.frame(data.xts))
# check the ordering of the spatialPointsDataFrame and the xts series
stopifnot(all(diff(match(point$name, colnames(as.data.frame(data.xts))))==1))
# data.frame for data
df <- data.frame(precip=as.vector(t(as.matrix(as.data.frame(data.xts)))))
stfdf <- STFDF(sp = point, time = index(Lux_precipitation_2010_2011_sm), data = df)
stfdf <- STFDF(sp = point, time = index(data.xts), data = df)
#
# author: B. Graeler
# organization: 52* North Initiative for Geospatial Open Source Software GmbH, Muenster, Germany
# date: 01.10.2018 - 07.05.2019
#
# ====================================================================================================
#' data.xts   : xts, time series
#' point      : SpatialPointsDataFrame, point geospatial domain of data.xts
#'
# ====================================================================================================
xts2STFDF <- function(data.xts, point){
require(spacetime)  # STFDF
# check the ordering of the spatialPointsDataFrame and the xts series
stopifnot(all(diff(match(point$name, colnames(as.data.frame(data.xts))))==1))
# data.frame for data
df <- data.frame(precip=as.vector(t(as.matrix(as.data.frame(data.xts)))))
stfdf <- STFDF(sp = point, time = index(data.xts), data = df)
return(stfdf)
}
#
# author: B. Graeler
# organization: 52* North Initiative for Geospatial Open Source Software GmbH, Muenster, Germany
# date: 01.10.2018 - 07.05.2019
#
# ====================================================================================================
#' data.xts   : xts, time series
#' point      : SpatialPointsDataFrame, point geospatial domain of data.xts
#'
# ====================================================================================================
xts2STFDF <- function(data.xts, point){
require(spacetime)  # STFDF
# check the ordering of the spatialPointsDataFrame and the xts series
stopifnot(all(diff(match(point$name, colnames(as.data.frame(data.xts))))==1))
# data.frame for data
df <- data.frame(data = as.vector(t(as.matrix(as.data.frame(data.xts)))))
stfdf <- STFDF(sp = point, time = index(data.xts), data = df)
return(stfdf)
}
my.stfdf <- xts2STFDF(data.xts = my.smoothed.ts, point = stations)
stplot(my.stfdf[,1:16,"data"])
my.stfdf@sp$id
my.stfdf@sp$elev_luref
stplot(my.stfdf[,"2011-12-16", drop = FALSE], mode = "ts")
data("Lux_boundary")
spplot(my.stfdf@sp, "elev_luref",
sp.layout=list(sp.polygons = boundary.Lux),
scales=list(arrows=FALSE),
xlim=bbox(boundary.Lux)[1,],
ylim=bbox(boundary.Lux)[2,])
spplot(my.stfdf@sp, "elev_luref",
sp.layout=list(sp.polygons = boundary.Lux),
scales=list(arrows=FALSE),
xlim=bbox(boundary.Lux)[1,],
ylim=bbox(boundary.Lux)[2,],
colorkey = list(space = "right"))
spplot(my.stfdf@sp, "elev_luref",
sp.layout=list(sp.polygons = boundary.Lux),
scales=list(arrows=FALSE),
xlim=bbox(boundary.Lux)[1,],
ylim=bbox(boundary.Lux)[2,],
colorkey = list(space = "rigth"))
spplot(my.stfdf@sp, "elev_luref",
sp.layout=list(sp.polygons = boundary.Lux),
scales=list(arrows=FALSE),
xlim=bbox(boundary.Lux)[1,],
ylim=bbox(boundary.Lux)[2,],
colorkey = list(space = "left"))
spplot(my.stfdf@sp, "elev_luref",
sp.layout=list(sp.polygons = boundary.Lux),
scales=list(arrows=FALSE),
xlim=bbox(boundary.Lux)[1,],
ylim=bbox(boundary.Lux)[2,])
spplot(my.stfdf@sp, "elev_luref",
sp.layout=list(sp.polygons = boundary.Lux),
scales=list(arrows=FALSE),
xlim=bbox(boundary.Lux)[1,],
ylim=bbox(boundary.Lux)[2,],
colorkey = list(space = "left", height = 0.4))
spplot(my.stfdf@sp, "elev_luref",
sp.layout=list(sp.polygons = boundary.Lux),
scales=list(arrows=FALSE),
xlim=bbox(boundary.Lux)[1,],
ylim=bbox(boundary.Lux)[2,],
key = list(space = "left", height = 0.4))
spplot(my.stfdf@sp, "elev_luref",
sp.layout=list(sp.polygons = boundary.Lux),
scales=list(arrows=FALSE),
xlim=bbox(boundary.Lux)[1,],
ylim=bbox(boundary.Lux)[2,],
key.space = "right")
rm(list=ls())
source("/home/atorres/Documents/02_working/3-Production/05_IV-Year/05_models/01_stUPscales/stUPscales/R")
source("/home/atorres/Documents/02_working/3-Production/05_IV-Year/05_models/01_stUPscales/stUPscales/R/")
source("~/Documents/02_working/3-Production/05_IV-Year/05_models/01_stUPscales/stUPscales/R/Correct_radar.R")
getwd()
prompt("Correct.radar")
prompt("Correct.radar.R")
source("~/Documents/02_working/3-Production/05_IV-Year/05_models/01_stUPscales/stUPscales/R/Smooth_ts.R")
prompt("Smooth.ts")
source("~/Documents/02_working/3-Production/05_IV-Year/05_models/01_stUPscales/stUPscales/R/xts2STFDF.R")
prompt("xts2STFDF")
data("Lux_stations")
data("Lux_precipitation_2010_2011")
data.xts <- Lux_precipitation_2010_2011
my.stfdf <- xts2STFDF(data.xts, point = stations)
stplot(my.stfdf[,1:16,"data"])
stplot(my.stfdf[,"2011-12-16", drop = FALSE], mode = "ts")
data("Lux_boundary")
spplot(my.stfdf@sp, "elev_luref",
sp.layout=list(sp.polygons = boundary.Lux),
scales=list(arrows=FALSE),
xlim=bbox(boundary.Lux)[1,],
ylim=bbox(boundary.Lux)[2,],
key.space = "right")
source("~/Documents/02_working/3-Production/05_IV-Year/05_models/01_stUPscales/stUPscales/R/IsReg_files.R")
prompt("IsReg.files")
##---------------------------------------------------------------------
## user's configuration (III)
##---------------------------------------------------------------------
## working directory
wdir <-  "/home/atorres/Documents/02_working/3-Production/05_IV-Year/05_models/01_stUPscales/stUPscales/"
## commit label
commit.label <- 'new radar and rain gauge data functions -- version 1100 started'
## commit label
commit.label <- 'new radar and rain gauge data functions -- version 1100 started'
## Gitlab repository
username   <- "Arturo.Torres"
password   <- "sehajsamadhi1A"
repository <- "git.list.lu/geocomputation/quics/stUPscales.git"
credential <- paste0("https://", username, ":", password, "@", repository)
##---------------------------------------------------------------------
## commit changes locally
##---------------------------------------------------------------------
## change to working directory
setwd(wdir)
## to see status of files
system("git status")
## to look previous status of commmits
system("git log")
## find and write big files in .gitignore
system("find . -size +50M | cat >> .gitignore")
## to stage files
system("git add .")
## to commit changes with label
system(paste0("git commit -a -m '", commit.label, "'"))
## to see status of files
system("git status")
## to look previous status of commmits
system("git log")
##---------------------------------------------------------------------
## configuring remote.origin.url for Github repository and pushing
##---------------------------------------------------------------------
system(paste0("git config remote.origin.url ", credential))
system(paste0("git push ", credential, " master"))
# ====================================================================================================
# Checking regular times in folder
# ====================================================================================================
library(stUPscales)
path     <- "/home/atorres/Documents/02_working/3-Production/05_IV-Year/09_datasets/02_Germany/HS_RW20111216_stfdf_DWD-calibrated-radar/RW20111216/"
IsReg.files(path = path, sep = "-", pattern=".tif", format = "%y%m%d%H%M")
