#' Set all paths to correctly and successfully run the Python API from within R
#'---------------------------------------------------------------------------------------------
#+ set_env, echo=TRUE, cache=FALSE, include=TRUE
set_env()
open_app()
library(RQGIS)
#' Set all paths to correctly and successfully run the Python API from within R
#'---------------------------------------------------------------------------------------------
#+ set_env, echo=TRUE, cache=FALSE, include=TRUE
set_env()
open_app()
qgis_env <-set_env(qgis_prefix_path = '/usr/share/qgis')
info_r <- version
info_qgis <- qgis_session_info()
qgis_env
qgis_env <-set_env(qgis_prefix_path = "/usr/share/qgis")
qgis_env
open_app()
qgis_env <-set_env(root = "/atorres")
qgis_env
qgis_env <-set_env(root = "atorres")
qgis_env
qgis_env <-set_env(root = "/atorres")
qgis_env
qgis_env <-set_env(root = "/usr/bin")
qgis_env
qgis_env <-set_env(root = "/atorres", new = TRUE)
qgis_env <-set_env(root = "/usr", new = TRUE)
qgis_env
open_app()
find_algorithms("slope(.+)?aspect")
open_app()
qgis_env <-set_env(root = "/usr", new = TRUE)
open_app()
qgis_env
open_app()
library(RQGIS)
#' Set all paths to correctly and successfully run the Python API from within R
#'---------------------------------------------------------------------------------------------
#+ set_env, echo=TRUE, cache=FALSE, include=TRUE
set_env()
open_app()
qgis_env <-set_env(root = "/usr")
qgis_env
open_app()
library(RQGIS)
#' Set all paths to correctly and successfully run the Python API from within R
#'---------------------------------------------------------------------------------------------
#+ set_env, echo=TRUE, cache=FALSE, include=TRUE
set_env()
open_app()
#' Set all paths to correctly and successfully run the Python API from within R
#'---------------------------------------------------------------------------------------------
#+ set_env, echo=TRUE, cache=FALSE, include=TRUE
set_env()
open_app()
library(RQGIS)
open_app()
#' Set all paths to correctly and successfully run the Python API from within R
#'---------------------------------------------------------------------------------------------
#+ set_env, echo=TRUE, cache=FALSE, include=TRUE
set_env()
open_app()
#' Set all paths to correctly and successfully run the Python API from within R
#'---------------------------------------------------------------------------------------------
#+ set_env, echo=TRUE, cache=FALSE, include=TRUE
set_env()
open_app()
library(RQGIS)
#' Set all paths to correctly and successfully run the Python API from within R
#'---------------------------------------------------------------------------------------------
#+ set_env, echo=TRUE, cache=FALSE, include=TRUE
set_env()
open_app()
#' Set all paths to correctly and successfully run the Python API from within R
#'---------------------------------------------------------------------------------------------
#+ set_env, echo=TRUE, cache=FALSE, include=TRUE
set_env()
open_app()
library(RQGIS)
#' Set all paths to correctly and successfully run the Python API from within R
#'---------------------------------------------------------------------------------------------
#+ set_env, echo=TRUE, cache=FALSE, include=TRUE
set_env()
open_app()
library(RQGIS)
#' Set all paths to correctly and successfully run the Python API from within R
#'---------------------------------------------------------------------------------------------
#+ set_env, echo=TRUE, cache=FALSE, include=TRUE
set_env()
open_app()
library(RQGIS)
#' Set all paths to correctly and successfully run the Python API from within R
#'---------------------------------------------------------------------------------------------
#+ set_env, echo=TRUE, cache=FALSE, include=TRUE
set_env()
open_app()
rgb(35)
rgb(0,0,0)
rgb(255,255,255)
library(tabulizer)
#' ---
#'title: "Bayesian Network Analysis"
#'author: "Simona Pedde & J.A. Torres-Matallana"
#'output            : pdf_document
#'fig_caption: true
#'---
#+ code, include = TRUE
# organization: CEH (UK)
# Wageningen University and Research Centre (WUR)
# date: 25.04.2019 - 25.04.2019
#' invoke rmarkdown
#'---------------------------------------------------------------------------------------------
#+ setup0, echo=FALSE, eval=FALSE, include=TRUE
library("rmarkdown")
rmarkdown::render("BayesianNetworks.R", pdf_document())
#' Setup by user
#'---------------------------------------------------------------------------------------------
#+ setup, echo=TRUE, include=TRUE
rm(list=ls())
library(bnlearn)
library(vars)
library(lars)
#' Loading data
#'---------------------------------------------------------------------------------------------
#+ data, echo=TRUE, include=TRUE
data(coronary)
dim(coronary)
head(coronary)
data("learning.test")
learning.test
#' learning structure
#'---------------------------------------------------------------------------------------------
#+ struct, echo=TRUE, include=TRUE
(bn_df <- data.frame(coronary))
class(bn_df)
(res <- hc(bn_df))
plot(res)
class(res)
(res.1 <- hc(learning.test))
plot(res.1)
#' Let’s remove the link between M.Work and Family.
res$arcs <- res$arcs[-which((res$arcs[,'from'] == "M..Work" &
res$arcs[,'to'] == "Family")),]
plot(res)
#' Training
#'---------------------------------------------------------------------------------------------
#+ training, echo=TRUE, include=TRUE
#' After learning the structure, we need to find out the
#' conditional probability tables (CPTs) at each node.
#' The bn.fit function runs the EM algorithm to learn CPT
#' for different nodes in the above graph.
fittedbn <- bn.fit(res, data = bn_df)
#' For example, let look at what is inside the Protein node.
print(fittedbn$Proteins)
#' Inference
#'---------------------------------------------------------------------------------------------
#+ inference, echo=TRUE, include=TRUE
#' Now, the BN is ready and we can start inferring from the network.
cpquery(fittedbn, event = (Proteins=="<3"), evidence = ( Smoking=="no") )
#' which results in 0.61. Note that although the Proteins variable
#' is conditioned on 2 variables, we did the query based on the
#' available evidence on only one variables.
#' But let make our evidence richer by asking the following:
#' What is the chance that a non-smoker with pressure greater
#' than 140 has a Proteins level less than 3?
cpquery(fittedbn, event = (Proteins=="<3"),
evidence = ( Smoking=="no" & Pressure==">140" ) )
#' Actual data UK
#'---------------------------------------------------------------------------------------------
#+ actual_data, echo=TRUE, include=TRUE
time  <- seq(from = 1990, to = 2015, by = 1)
set.seed(70)
var1 <- runif(n = length(time), min = 15, max = 50)
var1
var2 <- rnorm(n = length(time), mean = 100, sd = 0.15)
var2
var3 <- runif(n = length(time), min = 15, max = 100)
var3
var4 <- var2 * 10
var4
var5 <- var1 * 2
var5
var6 <- var1 * -2
var6
My.var <- function(var.1){
#' annual change (percentage)
diff <- diff(var.1)
perc <- diff/var.1[1:(length(var.1)-1)]*100
check <- matrix(data=NA, nrow=length(perc), ncol=1)
for(i in 1:(length(var.1)-1)){
if(perc[i] < 0){
check[i] <- var.1[i] - var.1[i]*(-perc[i]/100)
}
if(perc[i] >= 0){
check[i] <- var.1[i] + var.1[i]*(perc[i]/100)
}
}
data <- cbind.data.frame(time = time[1:length(time)-1],
data = var.1[1:length(time)-1],
perc = perc,
check = check)
data
range <- max(perc) - min(perc)
class <- matrix(data=NA, nrow=length(perc), ncol=1)
for(i in 1:length(perc)){
pos <- max(perc)/3
neg <- min(perc)/3
if(perc[i] > 0 & perc[i] < 1*pos){class[i] <- "slightly increase"}
else if(perc[i] >= 1*pos & perc[i] < 2*pos){class[i] <- "medium increase"}
else if(perc[i] >= 2*pos & perc[i] <= max(perc)){class[i] <- "high increase"}
else if(perc[i] < 0 & perc[i] > 1*neg){class[i] <- "slightly decrease"}
else if(perc[i] <= 1*neg & perc[i] > 2*neg){class[i] <- "medium decrease"}
else if(perc[i] <= 2*neg & perc[i] >= min(perc)){class[i] <- "high decrease"}
}
data <- cbind.data.frame(data, class=class)
data
}
#' e.g. red beef (number of cows in thousands)
(data.1 <- My.var(var1))
#' e.g. GDP  (in billions of dollars)
(data.2 <- My.var(var2))
#' e.g. arable land (in million Ha)
(data.3 <- My.var(var3))
#' e.g. arable crops1 (in Kg/Ha)
(data.4 <- My.var(var4))
#' e.g. arable crops2 (in Kg/Ha)
(data.5 <- My.var(var5))
#' e.g. arable crops3 (in Kg/Ha)
(data.6 <- My.var(var6))
data <- cbind.data.frame(var1 = factor(data.1[,5]),
var2 = factor(data.2[,5]),
var3 = factor(data.3[,5]),
var4 = factor(data.4[,5]),
var5 = factor(data.5[,5]),
var6 = factor(data.6[,5]))
colnames(data) <- c("var1", "var2", "var3", "var4", "var5", "var6")
(my.res <- hc(data))
plot(my.res)
# (my.res <- tabu(data))
# plot(my.res)
my.fittedbn <- bn.fit(my.res, data = data)
print(my.fittedbn$var1)
print(my.fittedbn$var2)
print(my.fittedbn$var3)
print(my.fittedbn$var4)
print(my.fittedbn$var5)
print(my.fittedbn$var6)
cpquery(my.fittedbn, event = (var1=="high increase"),
evidence = ( var2=="medium decrease" & var3=="medium decrease"))
cpquery(my.fittedbn, event = (var1=="high increase"),
evidence = ( var4=="medium decrease" & var5=="medium decrease"))
cpquery(my.fittedbn, event = (var1=="high increase"),
evidence = ( var4=="medium decrease" & var5=="high increase"))
#' Actual data UK (continuos data)
#'==============================================================================
#+ actual_data_raw, echo=TRUE, include=TRUE
data.raw <- cbind.data.frame(var1 = var1,
var2 = var2,
var3 = var3,
var4 = var4,
var5 = var5,
var6 = var6)
data.raw
(my.res.raw <- hc(data.raw))
plot(my.res.raw)
var.model = VAR(data.raw, p = 1, type = "const")
var.model
#' The estimated coefficient contains many missing values (NA); therefore,
#' approaches allowing for dimension reduction are required to analyze these data.
#' We consider the following dimension reduction approaches:
#'
#' • L 1 norm penalty (LASSO)
#'
#' • James-Stein shrinkage
#'
#' • Low-order conditional dependencies approximation
#'
#' LASSO with lars:
# library(GeneNet)
# data(arth800)
# summary(arth800.expr)
# dim(arth800.expr)
#' The data contains 2 sets of 11 time points.
# variance = diag(var(arth800.expr))
# plot(sort(variance, decreasing = TRUE),
#        type = "l", ylab = "Variance")
# abline(h = 2, lty = 2)
# posVar2 = which(variance > 2)
# dataVar2 = arth800.expr[, posVar2]
# dim(dataVar2)
# head(dataVar2)
# dataVar2inline = dataVar2[c(seq(1, 22, by = 2),
#                             seq(2, 22, by = 2)), ]
# head(dataVar2inline)
# dim(dataVar2inline)
#
# data = dataVar2inline
# x = data[-c(21:22), ]
(data.raw <- as.matrix(data.raw[]))
x = as.matrix(data.raw)
dim(x)
min(x)
fit.all = lapply(colnames(data.raw),
function(variable) {
y = data.raw[, variable]
lars(y = y, x = x, type = "lasso")
})
fit.all
cv.pred.all = lapply(1:dim(data.raw)[2],
function(variable) {
y = data.raw[, variable]
lasso.cv = cv.lars(y = y, x = x,
mode = "fraction")
frac = lasso.cv$index[
which.min(lasso.cv$cv)]
predict(fit.all[[variable]], s = frac,
type = "coef", mode = "fraction")
})
DBNlasso = matrix(0, dim(data.raw)[2], dim(data.raw)[2])
for (i in 1:dim(DBNlasso)[1]) {
DBNlasso[i, ] =
cv.pred.all[i][[1]]$coefficients
}
# percentage of arcs
sum(DBNlasso != 0)
sum(DBNlasso != 0)/prod(dim(DBNlasso))
plot(sort(abs(DBNlasso), decr = TRUE),
type = "l",
ylab = "Absolute coefficients")
DBNlasso
library(corrplot)
correl <- cor(data.raw)
corrplot(correl, type = "lower", method = "circle")
data.noncorrel <- cbind.data.frame(var1 = factor(data.1[,5]),
var2 = factor(data.2[,5]),
var3 = factor(data.3[,5]))
(my.res.noncorrel <- hc(data.noncorrel))
plot(my.res.noncorrel)
my.fittedbn.noncorrel <- bn.fit(my.res.noncorrel, data = data.noncorrel)
print(my.fittedbn.noncorrel$var1)
print(my.fittedbn.noncorrel$var2)
print(my.fittedbn.noncorrel$var3)
cpquery(my.fittedbn.noncorrel, event = (var1=="high increase"),
evidence = ( var2=="medium decrease" & var3=="medium decrease"))
cpquery(my.fittedbn.noncorrel, event = (var2=="high increase"),
evidence = ( var1=="medium decrease" & var3=="medium decrease"))
#' Assigning dependencies
#'==============================================================================
#+ assign_dependencies, echo=TRUE, include=TRUE
my.res.noncorrel <- set.arc(x = my.res.noncorrel, from =  "var1", to = "var3")
my.res.noncorrel <- set.arc(x = my.res.noncorrel, from =  "var2", to = "var1")
my.res.noncorrel
plot(my.res.noncorrel)
my.fittedbn.noncorrel2 <- bn.fit(my.res.noncorrel, data = data.noncorrel)
print(my.fittedbn.noncorrel2$var1)
print(my.fittedbn.noncorrel2$var2)
print(my.fittedbn.noncorrel2$var3)
cpquery(my.fittedbn.noncorrel, event = (var1=="high increase"),
evidence = ( var2=="medium decrease" & var3=="medium decrease"))
cpquery(my.fittedbn.noncorrel, event = (var2=="high increase"),
evidence = ( var1=="medium decrease" & var3=="medium decrease"))
bn.graph <- graphviz.chart(my.fittedbn.noncorrel2)
log(-89)
log(89)
log(,001)
log(.001)
library(RefManageR)
sessionInfo()
library(knitr)
library(RefManageR)
library(tm)
folder.base   <- "~/Documents/02_working/3-Production/01_bibliography/03_R/searches/"
file.bibtex   <- "~/Documents/02_working/3-Production/01_bibliography/03_R/searches/WOS_PhD_general_introduction.bib"
folder.new    <- ""
file.bibentry.new  <- ""
entry.key2remove  <- "ISI:000272073800017"
bibtex.file <- ReadBib(file.bibtex, .Encoding = "UTF-8")
library(knitr)
library(RefManageR)
library(tm)
sessionInfo()
folder.base   <- "~/Documents/02_working/3-Production/01_bibliography/03_R/searches/"
file.bibtex   <- "~/Documents/02_working/3-Production/01_bibliography/03_R/searches/WOS_PhD_general_introduction.bib"
folder.new    <- ""
file.bibentry.new  <- ""
entry.key2remove  <- "ISI:000272073800017"
bibtex.file <- ReadBib(file.bibtex, .Encoding = "UTF-8")
load("~/Documents/02_working/3-Production/01_bibliography/03_R/searches/WoS_PhD_general_introduction/output/ws_WOS_PhD_general_introduction.RData")
load("~/Documents/02_working/3-Production/01_bibliography/03_R/searches/WoS_PhD_general_introduction/output/WOS_PhD_general_introduction_net-citatition-stat-recent.RData")
str(stats)
stats
cor(stats)
cor(stats[-1])
stats[order(cluster.res1["Ranking"]),
c("vertexID", "Ranking", "cluster")]
stats[order(stats["Ranking"]),
c("vertexID", "Ranking", "cluster")]
stats[order(stats["Ranking"]),
c("vertexID", "Ranking", "vertexAuthority", "cluster")]
stats[order(stats["vertexAuthority"]),
c("vertexID", "Ranking", "vertexAuthority", "cluster")]
stats[order(stats["vertexCentrEigen"]),
c("vertexID", "Ranking", "vertexCentrEigen", "cluster")]
#' #' Saves plot as png on the filesytem.
#' #'
#'
library(EOTSA)
data(PROBAV_smpl)
plot(x=c(1:10), y=c(1:10))
my_plot <- recordPlot()
plot.new() ## clean up device
my_plot # redraw
my_path <- "my_plot"
eotsa_save_plot(the_plot = my_plot, filepath = my_path)
eotsa_save_plot(the_plot = my_plot, filepath = my_path)
my_path <- "/home/atorres/Documents/02_working_post-doc/3-Production/05_models/03_EOTSA/unit_tests/plots"
my_path <- "/home/atorres/Documents/02_working_post-doc/3-Production/05_models/03_EOTSA/unit_tests/plots"
eotsa_save_plot(the_plot = my_plot, filepath = my_path)
my_path <- "/home/atorres/Documents/02_working_post-doc/3-Production/05_models/03_EOTSA/unit_tests/plots.png"
eotsa_save_plot(the_plot = my_plot, filepath = my_path)
library(EOTSA)
data(PROBAV_smpl)
plot(rs)
#' eotsa_arima: (function) Calculates ARIMA coefficients for a given STFDF or RasterStack/-Brick object using auto.arima.
#' -----------------------------------------------------------------------------------------------------------
#'
#' Config
local <- TRUE
startdate <- "2015-01-01"
enddate   <- "2018-12-31"
bbox      <- "4, 4.05, 51.95, 52"
#' Loading lybrary
library(EOTSA)
librray(testthat)
#' Loading lybrary
library(EOTSA)
library(testthat)
#' Defining bounding box
bbox <- as.numeric(strsplit(bbox, split=",", fixed=TRUE)[[1]])
#' Data import
if (local) {
# Loading rs dataset (a RasterBrick)
data(PROBAV_smpl)
} else {
rs <- loadPROBA(startdate, enddate, extent(bbox))
}
#' Loading lybrary
library(EOTSA)
library(spacetime)
# ====================================================================================================
# Checking regular times in folder
# ====================================================================================================
library(stUPscales)
path     <- "/home/atorres/Documents/02_working/3-Production/05_IV-Year/09_datasets/02_Germany/HS_RW20111216_stfdf_DWD-calibrated-radar/RW20111216/"
IsReg.files(path = path, sep = "-", pattern=".tif", format = "%y%m%d%H%M")
path.calibrated     <- "/home/atorres/Documents/02_working/3-Production/05_IV-Year/09_datasets/02_Germany/HS_RW20111216_stfdf_DWD-calibrated-radar/RW20111216/"
path.non.calibrated <- "/home/atorres/Documents/02_working/3-Production/05_IV-Year/09_datasets/02_Germany/HS_RY20111216_stfdf_DWD-non-calibrated-radar/RY_20111216/"
myImgSTFDF <- Correct.radar(path.calibrated, path.non.calibrated,
time.ini.calib = "1112160055", format = "%y%m%d%H%M",
n = 18, NAcutoff = 250)
myImgSTFDF <- Correct.radar(path.calibrated, path.non.calibrated,
time.ini.calib = "1112160055", format = "%y%m%d%H%M",
n = 6, NAcutoff = 250)
options(error=recover)
myImgSTFDF <- Correct.radar(path.calibrated, path.non.calibrated,
time.ini.calib = "1112160055", format = "%y%m%d%H%M",
n = 6, NAcutoff = 250)
myImgSTFDF <- Correct.radar(path.calibrated, path.non.calibrated,
time.ini.calib = "1112160055", format = "%y%m%d%H%M",
n = 6, NAcutoff = 250)
# plot(imgSum, imgCalibrated$band1)
plot(hexbin(imgSum, imgCalibrated$band1))
plot(imgSum, imgCalibrated$band1)
hexbin(imgSum, imgCalibrated$band1)
plot(hexbin(imgSum, imgCalibrated$band1))
##---------------------------------------------------------------------
## user's configuration (III)
##---------------------------------------------------------------------
## working directory
wdir <-  "/home/atorres/Documents/02_working/3-Production/05_IV-Year/05_models/01_stUPscales/stUPscales/"
## commit label
commit.label <- 'excluded hexabin plot from Correction.radar.R '
## Gitlab repository
username   <- "Arturo.Torres"
password   <- "sehajsamadhi1A"
repository <- "git.list.lu/geocomputation/quics/stUPscales.git"
credential <- paste0("https://", username, ":", password, "@", repository)
##---------------------------------------------------------------------
## commit changes locally
##---------------------------------------------------------------------
## change to working directory
setwd(wdir)
## to see status of files
system("git status")
## to look previous status of commmits
system("git log")
## find and write big files in .gitignore
system("find . -size +50M | cat >> .gitignore")
## to stage files
system("git add .")
## to commit changes with label
system(paste0("git commit -a -m '", commit.label, "'"))
## to see status of files
system("git status")
## to look previous status of commmits
system("git log")
##---------------------------------------------------------------------
## configuring remote.origin.url for Github repository and pushing
##---------------------------------------------------------------------
system(paste0("git config remote.origin.url ", credential))
system(paste0("git push ", credential, " master"))
